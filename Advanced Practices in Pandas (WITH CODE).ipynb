{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS3 Kaggle Workshop - Advanced Practices in Pandas\n",
    "\n",
    "Welcome to our Advanced Practices in Pandas Jupyter Notebook. This is from our Advanced Practices in Pandas Workshop on November 24, 2020 from 7-8 PM PST. We'd love to see you at our workshop!\n",
    "\n",
    "With our interactive problems, we hope to guide you in your learning process. Here, you can practice useful pandas functions for DataFrame manipulation and analysis. Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will be using is called [“Uber and Lyft Cab Prices”](https://www.kaggle.com/ravi72munde/uber-lyft-cab-prices?select=weather.csv) from Kaggle. For your convenience, we have downloaded it into the same repository as this Jupyter Notebook for you.\n",
    "\n",
    "**Note:** The slideshow presentation will be published after the workshop. This will allow you to look back at the material covered and go over concepts that we were not able to get to during the timeframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_rides = pd.read_csv('cab_rides.csv')\n",
    "cab_rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('weather.csv')\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating Dataframes\n",
    "[`pd.concat()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) method: appends, or concatenates, two or more dataframes.\n",
    "* Can be concatenated vertically (one atop another), which is default (axis = 0)\n",
    "* Can also be concatenated horizontally (side-by-side) (axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument for `pd.concat()` is an Iterable with elements of type DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it make more sense to concatenate vertically or horizontally with these datsets?\n",
    "* Vertically (axis = 0)\n",
    "* Horizontally (axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis = 1 for concat horizontally\n",
    "pd.concat([cab_rides, weather], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do a little bit of work if we want to be able to use `pd.concat()` . Simply putting one table atop another, or side-by-side does not help us analyze our data. More specifically, if we want to concatenate the tables horizontally, simply doing the code in the previous cell will not work because the locations in the `cab_rides` rows and `weather` rows will not correspond to one another. There will be a mismatch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to concatenate the datasets, there needs to be commonality in the corresponding rows of the two dataframes. Moreover, the number of rows should be equal, otherwise you'll have a lot of NaN values after concatenating. Which column does it make sense to append from the two datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_rides.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_rides.time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.time_stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense to consolidate these two datasets on the basis of 'time_stamp' first because weather changes with time, and it would be nice to have the cab rides with the weather at the particular time. However, we encounter an issue. The 'time_stamp' values in both datasets are very long (and seemingly arbitrary) integers. However, after reading the notes from the creator of this dataset, we can learn that these times are expressed in Unix Epoch time. The `weather` times are in seconds, but the `cab_rides` times are in milliseconds, which is why they look different as well.\n",
    "\n",
    "\n",
    "Let's write a quick function `convert_unix_epoch_to_EST` to reformat Unix Epoch time to EST (Boston's time zone)! We will use `pd.apply()` to convert the times across all rows in both dataframes but we'll go deeper into `pd.apply()` later in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unix_epoch_to_EST(epoch_time_sec):\n",
    "    epoch_time_sec = epoch_time_sec - 5 * 60 * 60   # subtract 3 hours because EST is GMT/UTC -5\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(epoch_time_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_rides['time_stamp'] = cab_rides['time_stamp'] / 1000.0 # cab_rides timestamps: ms --> s\n",
    "cab_rides['time_stamp'] = cab_rides['time_stamp'].apply(convert_unix_epoch_to_EST) # apply function along column\n",
    "weather['time_stamp'] = weather['time_stamp'].apply(convert_unix_epoch_to_EST) # weather timestamps already in sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_rides.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find some basic summary statistics to see how well the `time_stamp` column in each dataframe matches up. Ideally, the minimum, maximum, and length (and all the values in the middle) will be the same if we want a perfect match. However, data is rarely perfect as we know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_rides.get('time_stamp').min(), cab_rides.get('time_stamp').max(), len(cab_rides.get('time_stamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.get('time_stamp').min(), weather.get('time_stamp').max(), len(weather.get('time_stamp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can foresee a problem if we choose to concatenate the dataframes. The `cab_rides` and `weather` timestamps will not match, and this will hinder our analysis. We can try concatenating and observe this. But first, we need to sort both dataframes by their timestamps, since they are not already sorted by that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_rides = cab_rides.sort_values(by = 'time_stamp').reset_index(drop = True)\n",
    "weather = weather.sort_values(by = 'time_stamp').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_rides.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = pd.concat([cab_rides, weather], axis = 1)\n",
    "concat.head(3)\n",
    "concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat[['time_stamp']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem that we predicted earlier: the timestamps for cab_rides and weather do not match, and the locations do not match either! Let's try another approach since `pd.concat()` does not seem to be effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Dataframes\n",
    "[`pd.merge()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) method: for combining 2 dataframes based on common data or indices\n",
    "* Similar functionality to the SQL joins\n",
    "* Much more flexible than `pd.concat()` or `pd.join()`\n",
    "* Mandatory parameters: `left` and `right` dataframes but tons of optional parameters that help you merge on specific criteria\n",
    "* Any matched columns with the same name will be listed only once in the merged dataframe to avoid repetition\n",
    "* Any matched columns with different names will both be present in the merged dataframe, but will still be merged \n",
    "   * You can drop redundant columns if you'd like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try merging using the `on` parameter, which will merge both dataframes on the same column(s) passed as a list. Here's the catch: Both dataframes **must** share a column with that same name, which is often not the case.\n",
    "\n",
    "After merging on 'time_stamp', does anyone notice any issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(cab_rides, weather, on = ['time_stamp']).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's best to join on location and the associated time for the two dataframes, we can use two more explicit, and often more useful, parameters: `left_on` and `right_on`. \n",
    "\n",
    "Since `cab_rides` has 2 location columns, one for where the ride started ('source') and one for where it ended ('destination'), we need to make a decision. We can choose to have the 'source' from `cab_rides` match the 'location' from `weather` because people often have to wait outside for their rides, so this may be more impactful than the weather at their destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rideshare_and_weather = pd.merge(cab_rides, weather, left_on = ['time_stamp', 'source'], \n",
    "                                 right_on = ['time_stamp', 'location']).head()\n",
    "rideshare_and_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mini lesson on Joins (more examples on joins on [this](https://medium.com/swlh/merging-dataframes-with-pandas-pd-merge-7764c7e2d46d) Medium article by Ravjot Singh):\n",
    "* The joins for `pd.merge()` have a very similar functionality to SQL joins.\n",
    "* Inner join (default for `pd.merge()`): merges on only the rows that match within the `on` column(s) for both dataframes. This leads to the loss of most data because you're only keeping what is matching.\n",
    "* Left join: will keep all the rows from the `left` dataframe even if it does not have a match from the right dataframe. Think about it like Inner (all matches) + all the remaining rows from the `left` dataframe. Rows from the `left` dataframe that do not match with a row from the `right` dataframe will have a NaN value for the column from the `right` dataframe.\n",
    "* Right join: the mirror image of left join! Inner (all matches) + all the remaining rows from the `right` dataframe.\n",
    "* Outer join: union of all rows from both dataframes, despite whether they match or not. This preserves most of the data, but leaves you with tons of NaN values.\n",
    "\n",
    "There are definitely tradeoffs between the join, so choose carefully which will benefit you! If you do not want to use inner join, use the `how` parameter and specify the type of join.\n",
    "\n",
    "Another useful parameter for `pd.merge()` is the `indicator` parameter. It will give a column called '_merge' and tell you what type of join that row was generated by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(cab_rides, weather, left_on = ['time_stamp', 'source'], \n",
    "         right_on = ['time_stamp', 'location'], how = 'outer', indicator = True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging on index: If the dataframes you're merging match on their indices, you can use `pd.merge()` for that too! Just use the `left_index` and `right_index` Boolean parameters. \n",
    "\n",
    "**NOTE**: You can use any combination: one of [`left_index`, `left_on`] and one of [`right_index`, `right_on`] if you happen to be matching the index of one dataframe with a column of the other dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(cab_rides, weather, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, `pd.concat()` is preferable over `pd.merge()` whenever you want to consolidate more than two dataframes. You cannot do this in `pd.merge()` unless you merge two dataframes and then merge a third dataframe to the merged dataframe, which often gets complicated. However, when using `pd.concat()`, the data has to perfectly match up, which is not very likely in real-world data. Mismatched data could be detrimental to our analysis and we wouldn't even know it if our dataset is huge! Although `pd.concat()` does have SQL join functionality, using `pd.merge()` requires less preparation on your end before the consolidation.\n",
    "\n",
    "There is another way to consolidate two dataframes that is good to know. This is [`pd.join()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.join.html), which is used to join 2 dataframes on their indices. This is a more rigid function, so `pd.merge()` is used more. We have also learned that we can use `pd.merge()` for joining on indices.\n",
    "\n",
    "For more information on ways to consolidate dataframes, you can check out this [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) page!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose\n",
    "[`pd.transpose()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transpose.html) method: reflects the dataframe over its main diagonal by writing rows as columns and vice-versa\n",
    "\n",
    "\"Transpose\" means the process of exchanging places. This makes sense because we are exchanging places of the rows and columns (ie. interchanging the axes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_rides.shape # almost 70,000 rows and 10 columns !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you were to call transpose on the whole dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commented so it doesn't take forever to run. \n",
    "# feel free to uncomment if you want to try the line!\n",
    "\n",
    "# cab_rides.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... feels like it's running forever right? I would suggest taking this as a sign of NOT to call transpose on large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's work with a smaller dataset and call transpose on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_cabs= cab_rides.head()\n",
    "five_cabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_cabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_cabs_transpose = five_cabs.transpose() #same as .T\n",
    "five_cabs_transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_cabs_transpose.shape # see how the rows and columns switched?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the weather dataset? Seems much smaller than cab_rides. Let's call transpose on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_transpose = weather.T # same as .transpose()\n",
    "weather_transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when you call transpose on a series instead of a dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_cab_names = five_cabs['name']\n",
    "five_cab_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_cab_names.transpose() # looks the same to me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does a series behave this way? \n",
    "\n",
    "Series don't have the (row, column) format that dataframes do, so it just returns the same object. See below -- the column entry is empty!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_cab_names.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Problem: Transpose\n",
    "Let's go back to the dataset, rideshare_and_weather, which you created in the last segment where you learned merge. You created this dataset by taking the first five entries of the cab_rides and weather merge. Practice transpose on that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rideshare_and_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rideshare_and_weather.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`pd.groupby()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) method: splits the object, applies a function, and combines the results (also referred to as split-apply-combine strategy)\n",
    "\n",
    "`pd.groupby()` is a useful method often used in data science to group large amounts of data and compute operations on them.\n",
    "\n",
    "Let's call `pd.groupby()` on `cab_rides` and pass in the parameter `cab_type` and see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_rides.groupby('cab_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `pd.groupby()` on the dataframe doesn't perform any operations; it just returns a DataFrameGroupBy object. In order to make use of `pd.groupby()`, you need to add some type of aggregate function to it.\n",
    "\n",
    "Some aggregate functions you can use are: sum, mean, min, max.\n",
    "\n",
    "Let's call the `sum` function on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_rides.groupby('cab_type').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cab_rides.groupby('cab_type') tells us that we are grouping the data in the cab_rides dataset by the cab_type. You can see that the indices on the newly formed table are in fact different cab types. By calling .sum(), it takes the sum of each column based on cab_type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why aren't all columns returned? Some columns don't make sense to be aggregated. For example, the column 'destination' was taken out because it didn't make sense to take the sum of 'destination.'\n",
    "\n",
    "So let's take it one more step. How would you find the total distance (miles) done based on cab_type with the small dataframe you have above?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total distance miles done based on cab_type\n",
    "sum_distance = cab_rides.groupby('cab_type')[['distance']].sum()\n",
    "sum_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the total fare based on cab type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total money spent on cab_type\n",
    "sum_price = cab_rides.groupby('cab_type')[['price']].sum()\n",
    "sum_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Problem: Groupby\n",
    "Practicing groupby on the weather dataset, how would you get the averages (mean) for each location?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averages based on location\n",
    "weather.groupby('location').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you get just the average (mean) temperature based on location?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average temperature based on location\n",
    "weather.groupby('location')[['temp']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can elevate your `pd.groupby()` by adding on an apply function. This allows you to define and run your own custom function on the dataset, as opposed to using one of Pandas's pre defined aggregate functions. We will touch upon this later in the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply\n",
    "Let's get into the details of `pd.apply()` which enables us to apply a function along an axis of a DataFrame.\n",
    "\n",
    "* Objects passed to the function are Series objects whose index is either the DataFrame’s index (axis=0) or the DataFrame’s columns (axis=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toft(df):\n",
    "    ft = df * 5280\n",
    "    return ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miles = cab_rides[['distance']]\n",
    "feet = miles.apply(toft)\n",
    "\n",
    "#reassign to original dataframe \n",
    "cab_rides = cab_rides.assign(distance_ft = feet)\n",
    "cab_rides.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_rides['distance_ft2'] = cab_rides['distance'].apply(toft)\n",
    "cab_rides.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_rides = cab_rides.drop(columns = 'distance_ft2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda\n",
    " `pd.apply(lambda x:())` functions are defined using the keyword lambda. The function has only one expression and any number of arguements. A lambda function cannot contain any statements, and it returns a function object which can be assigned to any variable.\n",
    "\n",
    "*  Allows for application of a custom function to satisy a goal during data manipulation such as developing a unique DataFrame column.\n",
    "*  Useful for small tasks with less code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeofday(x):\n",
    "     if  x < 12:\n",
    "        return 'Morning'\n",
    "     elif 12<= x <18:\n",
    "        return \"Afternoon\"\n",
    "     else:\n",
    "        return \"Evening\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cab_rides['time_stamp'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabtime= pd.to_datetime(cab_rides['time_stamp'])\n",
    "dft = pd.DataFrame(data=cabtime)\n",
    "dft #new DataFrame time in datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_rides['time_stamp'] = dft['time_stamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()\n",
    "cab_rides['time_of_day'] = cab_rides.time_stamp.dt.hour.apply(lambda row: timeofday(row))\n",
    "b= time.time()\n",
    "b-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_rides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing\n",
    "When analyzing data, always have a plan of action. A difficult route could cost you a lot of time. Let's look at two ways of simplifying a variable for ease of use and further application.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tod(time):\n",
    "    t = time.str[11:13]\n",
    "    tint = [int(x) for x in t]\n",
    "    for i, x in enumerate(tint):\n",
    "        if  x < 12:\n",
    "             tint[i] = 'Morning'\n",
    "        elif 12<= x <18:\n",
    "            tint[i] = \"Afternoon\"\n",
    "        else:\n",
    "            tint[i] = \"Evening\"\n",
    "    return tint\n",
    "\n",
    "weather.time_stamp.str[11:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()\n",
    "weather['Timeofday'] = tod(weather['time_stamp'])\n",
    "b= time.time()\n",
    "b-a, weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['time_stamp'] = pd.to_datetime(weather['time_stamp'])\n",
    "a = time.time()\n",
    "weather['timeofday'] = weather['time_stamp'].dt.hour.apply(lambda row : timeofday(row))\n",
    "b = time.time()\n",
    "b-a, weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the calculation times of the two methods were quite similar, the actual time to code was much longer (double in this case !!!). Make sure to understand the particular data type you are working with and various functions available to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Rideshare\n",
    "Obviously sometimes you have to Uber and cannot wait for the time to be just right. Despite this fact, let's attempt to find the optimal weather to Uber or Lyft in terms of surge using the  `rideshare_and_weather` DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rideshare_and_weather = pd.merge(cab_rides, weather, left_on = ['time_stamp', 'source'], right_on = ['time_stamp', 'location'])\n",
    "rideshare_and_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surge_type(x):\n",
    "    if x > 1.0:\n",
    "        return 'SURGE'\n",
    "    else:\n",
    "        return 'Normal'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rideshare_and_weather['surge_type']= rideshare_and_weather.apply(lambda x: surge_type(x['surge_multiplier']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surge = rideshare_and_weather[rideshare_and_weather['surge_type'] == 'SURGE'].sort_values(by = 'time_stamp')\\\n",
    ".reset_index(drop = True)\n",
    "normal = rideshare_and_weather[rideshare_and_weather['surge_type'] == 'Normal'].sort_values(by = 'time_stamp')\\\n",
    ".reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the surge and normal rows to determine if their is an obvious discrepency between the two at the unique time of day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = surge.groupby('timeofday').mean()\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = normal.groupby('timeofday').mean()\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "a_heights, a_bins = np.histogram(surge['temp'])\n",
    "b_heights, b_bins = np.histogram(normal['temp'].sample(n=150, random_state=1), bins=a_bins)\n",
    "\n",
    "width = (a_bins[1] - a_bins[0])/3\n",
    "\n",
    "ax.bar(a_bins[:-1], a_heights, width=width, facecolor='cornflowerblue')\n",
    "ax.bar(b_bins[:-1]+width, b_heights, width=width, facecolor='seagreen')\n",
    "#seaborn.despine(ax=ax, offset=10)\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Count')\n",
    "fig.legend(['surge','normal'],bbox_to_anchor =(0.75, 1), ncol = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "a_heights, a_bins = np.histogram(surge['distance'])\n",
    "b_heights, b_bins = np.histogram(normal['distance'].sample(n=150, random_state=1), bins=a_bins)\n",
    "\n",
    "width = (a_bins[1] - a_bins[0])/3\n",
    "\n",
    "ax.bar(a_bins[:-1], a_heights, width=width, facecolor='cornflowerblue')\n",
    "ax.bar(b_bins[:-1]+width, b_heights, width=width, facecolor='seagreen')\n",
    "#seaborn.despine(ax=ax, offset=10)\n",
    "plt.xlabel('Distance (miles)')\n",
    "plt.ylabel('Count') \n",
    "fig.legend(['surge','normal'],bbox_to_anchor =(0.75, 1), ncol = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure() # Create matplotlib figure\n",
    "\n",
    "ax = fig.add_subplot(111) # Create matplotlib axes\n",
    "ax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.\n",
    "\n",
    "width = 0.4\n",
    "\n",
    "normal['timeofday'].sample(n=145, random_state=1).value_counts().plot(kind='bar', color='red', ax=ax, width=width, position=1)\n",
    "surge['timeofday'].value_counts().plot(kind='bar', color='blue', ax=ax2, width=width, position=0)\n",
    "\n",
    "ax.set_ylabel('Number')\n",
    "ax.set_ylim([0,70]);\n",
    "ax2.set_ylim([0,70]);\n",
    "fig.legend(['normal','surge'],bbox_to_anchor =(0.75, 1.15), ncol = 2)\n",
    "\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine under what circumstances the surge_multiplier is applied, one would need more data. Plain and simple. Whether surge is related to weather or the time of day , at this point, it is uncertain. All we can do is assume and extrapolate our assumption. However, this is dangerous as human bias is a \"guesstimate.\" \n",
    "\n",
    "*On your own* : Try a hypothesis test \n",
    "\n",
    "\n",
    "Possible ideas: \n",
    "- How does it seem like the number of rides are affected by the type of weather?\n",
    "- Is there a relation between the surge multiplier of the ride and the weather quality?\n",
    "- Does the length of the rides increase due to the poor weather quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
